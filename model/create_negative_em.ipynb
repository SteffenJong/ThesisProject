{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eacdc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing torch\n",
      "importing evo2\n",
      "importing partial\n",
      "importing hugging face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jong505/conda/miniconda3/envs/evo2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing pkgutil\n",
      "importing torch\n",
      "importing typing\n",
      "importing yaml\n",
      "importing vortex1\n",
      "importing vortex2\n",
      "importing vortex3\n",
      "importing vortex4\n",
      "importing evo2 scoring\n",
      "importing evo2 util\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from create_embeddings import create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_df(merged: pd.DataFrame, refseq: list):\n",
    "    #output dict.\n",
    "    output = {}\n",
    "    # loop trough reference sequences.\n",
    "    for file in refseq:\n",
    "        #open refseq \n",
    "        with gzip.open(file, \"rt\") as f:\n",
    "            # loop trough files in refseq (wich are mostly chromosones).\n",
    "            for record in tqdm(SeqIO.parse(f, format=\"fasta\"), desc=f\"Collecting sequences from{file}\"):\n",
    "                # get the name of the current name of the refseq (Wich genome we are looking at).\n",
    "                file_name = file.stem.split(\".\")[0]\n",
    "                # get the name of the current file in the refseq (Wich chromosone we are looking at).\n",
    "                chr_name = record.id.split(\".\")[0]\n",
    "                # filter the df such that we are only looking at rows that have a segment that is from the current genome and chromosone.\n",
    "                # We are doing this to reduce the size of the forloop, to improve speed.\n",
    "                df_focus = merged[(merged[[\"genome_x\", \"genome_y\"]] == file_name).any(axis=1) & (merged[[\"list_x\", \"list_y\"]] == chr_name).any(axis=1)]\n",
    "                # loop over df\n",
    "                for indx, r in df_focus.iterrows():\n",
    "                    # for the current row see if genome_x and or genome_y is the genome that is opened right now.\n",
    "                    genome = r[r==file_name]\n",
    "                    # loop over genome_x or genome_y or both debending on what genome is open right now.\n",
    "                    for i, _ in genome.items():\n",
    "                        # set x_y to either x or y depending on wich genome we are looking at\n",
    "                        x_y = i.split(\"_\")[1]\n",
    "                        # check if the current (iadh) segment is from the correct chromosone that we have opend right now.\n",
    "                        if r[f\"list_{x_y}\"] == chr_name:\n",
    "                            # see if a dict already exists for the current id(indx), if so we update it, if not we create the id and write the inital data.\n",
    "                            output.setdefault(r[\"id\"], {}).update({f\"genome_{x_y}\": r[f\"genome_{x_y}\"],\n",
    "                                                                f\"chr_{x_y}\": r[f\"list_{x_y}\"],\n",
    "                                                                f\"segment_id_{x_y}\": r[f\"segment_id_{x_y}\"],\n",
    "                                                                f\"len_profile_{x_y}\": r[f\"len_profile_{x_y}\"],\n",
    "                                                                f\"seq_{x_y}\": str(record.seq[r[f\"start_{x_y}\"]-1:r[f\"stop_{x_y}\"]])})\n",
    "                            \n",
    "    return pd.DataFrame.from_dict(output, orient='index')[[\"genome_x\", \"chr_x\", \"len_profile_x\", \"segment_id_x\", \"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_x\", \"seq_y\"]]\n",
    "\n",
    "def genes_to_fam(df, gene_fam):\n",
    "    # create series to map gene to fam\n",
    "    gene_to_fam = gene_fam.set_index(\"gene\")[\"fam\"]\n",
    "    df_x = (\n",
    "        df[[\"genes_x\"]]\n",
    "        # instead of having i1: [g1, g2 etc] we go to\n",
    "        # i1: g1\n",
    "        # i1: g2\n",
    "        .explode(\"genes_x\")\n",
    "        # add column fam for wich we map gene_x using gene to fam\n",
    "        .assign(fam=lambda df: df[\"genes_x\"].map(gene_to_fam))\n",
    "        # undo the explode\n",
    "        .groupby(level=0)[\"fam\"]\n",
    "        # turn every group by section into list\n",
    "        .agg(list)\n",
    "        # rename colum fam\n",
    "        .rename(\"fams_x\")\n",
    "    )\n",
    "\n",
    "    df_y = (\n",
    "        df[[\"genes_y\"]]\n",
    "        .explode(\"genes_y\")\n",
    "        .assign(fam=lambda df: df[\"genes_y\"].map(gene_to_fam))\n",
    "        .groupby(level=0)[\"fam\"]\n",
    "        .agg(list)\n",
    "        .rename(\"fams_y\")\n",
    "    )\n",
    "    return df.join([df_x, df_y])\n",
    "\n",
    "def create_test_df_og(train_df: pd.DataFrame):\n",
    "    df_c = train_df.copy() \n",
    "    df_c[\"og_index\"] = df_c.index\n",
    "\n",
    "    grouped = df_c.groupby([\"len_profile_x\", \"len_profile_y\"])\n",
    "    shuffled_parts = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        # makes sure group is not to small to shuffel\n",
    "        if len(group) == 1:\n",
    "            shuffled_parts.append(group)\n",
    "            print(\"Error group to small !\")\n",
    "            continue\n",
    "        \n",
    "        print(group[\"seq_y\"].duplicated().sum(),\"-\",(group.shape[0]/2))\n",
    "\n",
    "        if group[\"seq_y\"].duplicated(keep=False).sum() > (group.shape[0]/2):\n",
    "            print(\"Error group to similar !\")\n",
    "            continue\n",
    "\n",
    "        shuffled_group = group.copy()\n",
    "        same = True\n",
    "        # shuffel everything till there are no same combinations as before\n",
    "\n",
    "\n",
    "        while same:\n",
    "            shuffled_values = shuffled_group[[\"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_y\"]].sample(frac=1, random_state=None)\n",
    "            shuffled_group[[\"genome_y\", \"chr_y\", \"len_profile_y\",\"segment_id_y\", \"seq_y\"]] = shuffled_values[[\"genome_y\", \"chr_y\", \"len_profile_y\",\"segment_id_y\", \"seq_y\"]].values\n",
    "            \n",
    "            if shuffled_group[shuffled_group[\"seq_y\"] == group[\"seq_y\"]][\"seq_y\"].notna().sum() == 0:\n",
    "                same = False\n",
    "            \n",
    "        shuffled_parts.append(shuffled_group)\n",
    "    return pd.concat(shuffled_parts).sort_values(\"og_index\").drop(columns=\"og_index\")\n",
    "\n",
    "def create_train_test_val(df):\n",
    "    x_index, x_val_index, _, _ = train_test_split(df.index, df.similar.astype(int).values, test_size=0.30, random_state=42)\n",
    "    df_val = df.iloc[x_val_index]\n",
    "    df = df.iloc[x_index].reset_index(drop=True)\n",
    "    x_train_index, x_test_index, _, _ = train_test_split(df.index, df.similar.astype(int).values, test_size=0.20, random_state=42)\n",
    "    return df.iloc[x_train_index], df.iloc[x_test_index], df_val\n",
    "\n",
    "def filter_df(merged, seg_len):\n",
    "    df = pd.read_csv(merged, sep=\"\\t\", header=0)\n",
    "    df = df[(df[\"len_profile_x\"]<=seg_len) & (df[\"len_profile_y\"]<=seg_len) & (df[\"genome_x\"] != df[\"genome_y\"])]\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b971c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_elements_path = Path(\"/home/jong505/thesis/iadh/iadh_out/aar_ath_bol_chi_cpa_tha/list_elements.txt\")\n",
    "# list_elements = pd.read_csv(list_elements, sep='\\t', header=0)\n",
    "\n",
    "merged_path = Path(\"/home/jong505/thesis/iadh/iadh_out/aar_ath_bol_chi_cpa_tha/merged_results.tsv\")\n",
    "seg_len = 7\n",
    "# merged = pd.read_csv(merged_path, sep=\"\\t\", header=0)\n",
    "# merged = merged[(merged[\"len_profile_x\"]<=seg_len) & (merged[\"len_profile_y\"]<=seg_len) & (merged[\"genome_x\"] != merged[\"genome_y\"])]\n",
    "\n",
    "\n",
    "data_p = Path(\"/home/jong505/thesis/iadh/data\")\n",
    "refseqs = [f\"{data_p}/ath.fasta.gz\", f\"{data_p}/aar.fasta.gz\", f\"{data_p}/bol.fasta.gz\", f\"{data_p}/tha.fasta.gz\", f\"{data_p}/chi.fasta.gz\", f\"{data_p}/cpa.fasta.gz\"]\n",
    "refseqs = [Path(f) for f in refseqs]\n",
    "\n",
    "gene_fam_path = Path(\"/home/jong505/thesis/iadh/data/gene_fam_parsed.tsv\")\n",
    "# gene_fam = pd.read_csv(gene_fam_path, sep=\"\\t\", header=None,  names=['gene', 'fam'])\n",
    "\n",
    "output_prefix = Path(\"data/aar_ath_bol_chi_cpa_tha/sm7_50000_new_neg\")\n",
    "output_prefix_seq = Path(\"data/aar_ath_bol_chi_cpa_tha/sm7_50000_new_neg\")\n",
    "len_nuc = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bcab4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting sequences from/home/jong505/thesis/iadh/data/ath.fasta.gz: 7it [00:01,  5.28it/s]\n",
      "Collecting sequences from/home/jong505/thesis/iadh/data/aar.fasta.gz: 2883it [00:06, 430.58it/s]\n",
      "Collecting sequences from/home/jong505/thesis/iadh/data/bol.fasta.gz: 129it [00:05, 23.30it/s]\n",
      "Collecting sequences from/home/jong505/thesis/iadh/data/tha.fasta.gz: 12249it [00:22, 536.16it/s]\n",
      "Collecting sequences from/home/jong505/thesis/iadh/data/chi.fasta.gz: 624it [00:02, 227.19it/s]\n",
      "Collecting sequences from/home/jong505/thesis/iadh/data/cpa.fasta.gz: 5901it [00:12, 471.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for max length of 50000 nucliotides\n",
      "Went fom 1545 to 1323 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filter_df(merged_path, seg_len)\n",
    "\n",
    "# Only put samples in there that have the same orientation. \n",
    "# filtered_df = filtered_df[ (filtered_df[\"sim_orientations_x\"] >= 1) & (filtered_df[\"sim_orientations_y\"] >= 1)]\n",
    "\n",
    "train_df = create_train_df(filtered_df, refseqs)\n",
    "\n",
    "if len_nuc != 0:\n",
    "    print(f\"Filtering for max length of {len_nuc} nucliotides\")\n",
    "    before = train_df.shape[0]\n",
    "    train_df = train_df[train_df.apply(lambda r: max(len(r['seq_x']), len(r['seq_y'])) < len_nuc, axis=1)]\n",
    "    print(f\"Went fom {before} to {train_df.shape[0]} pairs\")\n",
    "\n",
    "# print(\"Creating negative samples\")\n",
    "# test_df = create_test_df(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6245f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_samples(df, gene_fam_path, list_elements):\n",
    "    gene_fam = pd.read_csv(gene_fam_path, sep=\"\\t\", header=None,  names=['gene', 'fam'])\n",
    "    list_elements = pd.read_csv(list_elements, sep='\\t', header=0)\n",
    "    # create series that can be used to map segment to gene\n",
    "    segment_to_gene = list_elements.set_index(\"segment\")[\"gene\"]\n",
    "    # aggregate from:\n",
    "    # seg1: g1\n",
    "    # seg1: g2\n",
    "    # to: seg1: [g1, g2]\n",
    "    segment_to_gene = segment_to_gene.groupby(\"segment\").agg(list)\n",
    "    # add collumn named gene_x in wich we use the segment id to look up what genes are in the segment using segment_to_gene\n",
    "    df = df.assign(genes_x=lambda df: df[\"segment_id_x\"].map(segment_to_gene))\n",
    "    df = df.assign(genes_y=lambda df: df[\"segment_id_y\"].map(segment_to_gene))\n",
    "    df = genes_to_fam(df, gene_fam)\n",
    "\n",
    "    # df_c = df.copy()\n",
    "    same = True\n",
    "\n",
    "    shuffled_df = df.copy().reset_index(drop=True)\n",
    "    # display(shuffled_df.head())\n",
    "    limit = 5000\n",
    "    counter = 0\n",
    "    lowest = 99999999999999\n",
    "    best = \"\"\n",
    "    while same:\n",
    "        if counter>= limit:\n",
    "            print(\"exeeded limt, failed to find new shuffel\")\n",
    "            same=False\n",
    "        counter+=1\n",
    "        \n",
    "        shuffled_df[[\"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_y\", \"fams_y\"]] = df[[\"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_y\", \"fams_y\"]].sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "        \n",
    "        shuffled_df[\"sim\"] = [bool(set(x) & set(y)) for x, y in zip(shuffled_df[\"fams_x\"], shuffled_df[\"fams_y\"])]\n",
    "        if shuffled_df.sim.sum()< lowest:\n",
    "            best = shuffled_df.copy()\n",
    "            lowest = shuffled_df.sim.sum()\n",
    "\n",
    "        if True not in [bool(set(x) & set(y)) for x, y in zip(df[\"fams_x\"], shuffled_df[\"fams_y\"])]:\n",
    "            return shuffled_df\n",
    "        \n",
    "        # shuffled_df[shuffled_df.sim][[\"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_y\", \"fams_y\"]] = shuffled_df[shuffled_df.sim][[\"genome_y\", \"chr_y\", \"len_profile_y\", \"segment_id_y\", \"seq_y\", \"fams_y\"]].sample(frac=1, random_state=None)\n",
    "\n",
    "    print(f\"dropping {best.sim.sum()} samples because of sim\")\n",
    "    return best[~best.sim]\n",
    "\n",
    "def get_from_dif_fam(fams, pool: pd.DataFrame, limit=1000):\n",
    "    for i in range(1000):\n",
    "        new = pool.sample(n=1)\n",
    "        if bool(set(new[\"fams_y\"]) & set(fams)):\n",
    "            pool.drop(index=new.index, inplace=True)\n",
    "            return new, pool\n",
    "    print(\"No new sample was found\")\n",
    "    return None, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75b4a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exeeded limt, failed to find new shuffel\n",
      "dropping 2 samples because of sim\n"
     ]
    }
   ],
   "source": [
    "test_df = create_negative_samples(train_df, gene_fam_path, list_elements_path)\n",
    "test_df[\"sim\"] = [bool(set(x) & set(y)) for x, y in zip(test_df[\"fams_x\"], test_df[\"fams_y\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209222c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"multiplicon_id\"] = pd.to_numeric(train_df.index, downcast='integer')\n",
    "test_df[\"multiplicon_id\"] = pd.NA\n",
    "train_df[\"similar\"] = True\n",
    "test_df[\"similar\"] = False\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "# print(f\"Writing output to {output_prefix}\")\n",
    "# df[[\"segment_id\", \"similar\", \"genome_x\", \"chr_x\", \"len_profile_x\", \"genome_y\", \"chr_y\", \"len_profile_y\", \"seq_x\", \"seq_y\"]].to_csv(output_path, sep='\\t')\n",
    "\n",
    "train, test, val = create_train_test_val(df)\n",
    "print(f\"shapes: train:{train.shape}, train:{test.shape}, train:{val.shape}\")\n",
    "\n",
    "train.drop(columns=[\"seq_x\", \"seq_y\"]).to_csv(str(output_prefix)+\"_train.tsv\", sep=\"\\t\")\n",
    "test.drop(columns=[\"seq_x\", \"seq_y\"]).to_csv(str(output_prefix)+\"_test.tsv\", sep=\"\\t\")\n",
    "val.drop(columns=[\"seq_x\", \"seq_y\"]).to_csv(str(output_prefix)+\"_val.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "if output_prefix_seq != \"\":\n",
    "    train.to_csv(str(output_prefix_seq)+\"_train_seq.tsv\", sep=\"\\t\")\n",
    "    test.to_csv(str(output_prefix_seq)+\"_test_seq.tsv\", sep=\"\\t\")\n",
    "    val.to_csv(str(output_prefix_seq)+\"_val_seq.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(\"Generating embeddings\")\n",
    "embeddings = create_embeddings(df)\n",
    "embeddings.to_csv(str(output_prefix_seq)+\"_embeddings.tsv\", sep=\"\\t\")\n",
    "\n",
    "# print(f\"generating train embeddings\")\n",
    "# train_em = create_embeddings(train)\n",
    "# print(train_em.head())\n",
    "# print(str(output_prefix)+\"_train.tsv\")\n",
    "# train_em.to_csv(str(output_prefix)+\"_train.tsv\", sep=\"\\t\")\n",
    "\n",
    "# print(f\"generating test embeddings\")\n",
    "# test_em = create_embeddings(test)\n",
    "# test_em.to_csv(str(output_prefix)+\"_test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# print(f\"generating val embeddings \")\n",
    "# val_em = create_embeddings(val)\n",
    "# val_em.to_csv(str(output_prefix)+\"_val.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
